# -*- coding: utf-8 -*-
"""Train data split and model building .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a2yICkz-NgokL9pfoRb_sUn9ZtahgCZ2
"""

import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
import joblib

# 1. Load Cleaned Data
DATA_PATH = "transactions_clean.csv"
df = pd.read_csv(DATA_PATH)
print("Dataset Loaded")
print(f"Rows: {len(df)}, Columns: {df.shape[1]}")

# 2. Prepare Features & Target
drop_cols = ["transaction_id", "customer_id", "timestamp"]
X = df.drop(columns=[col for col in drop_cols if col in df.columns] + ["is_fraud"], errors="ignore")
y = df["is_fraud"]

print(f"Feature matrix shape: {X.shape}")
print(f"Target imbalance: {y.value_counts(normalize=True).to_dict()}")

# 3. Split into Train / Validation / Test
X_train_full, X_test, y_train_full, y_test = train_test_split(
    X, y, test_size=0.15, random_state=42, stratify=y
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_full, y_train_full, test_size=0.1765, random_state=42, stratify=y_train_full
)

print(f"Split sizes -> Train: {len(X_train)}, Validation: {len(X_val)}, Test: {len(X_test)}")

# 4. Save Split Data
os.makedirs("data/processed", exist_ok=True)
for name, data_x, data_y, fname in [
    ("Train", X_train, y_train, "train.csv"),
    ("Validation", X_val, y_val, "validation.csv"),
    ("Test", X_test, y_test, "test.csv"),
]:
    df_split = data_x.copy()
    df_split["is_fraud"] = data_y.values
    df_split.to_csv(f"data/processed/{fname}", index=False)
    print(f"Saved {name} -> data/processed/{fname}")

# 5. Initialize Models (with Class Balancing)
models = {
    "Logistic Regression": LogisticRegression(max_iter=200, class_weight="balanced", solver="liblinear"),
    "Random Forest": RandomForestClassifier(
        n_estimators=200, random_state=42, class_weight="balanced_subsample"
    ),
    "XGBoost": XGBClassifier(
        n_estimators=200,
        learning_rate=0.05,
        max_depth=5,
        scale_pos_weight=(y_train.value_counts()[0] / y_train.value_counts()[1]),
        random_state=42,
        eval_metric="logloss",
        use_label_encoder=False
    )
}

# 6. Train, Validate, and Test Each Model
results = []

for name, model in models.items():
    print(f"\nTraining {name}...")
    model.fit(X_train, y_train)

    # Validation performance
    y_val_pred = model.predict(X_val)
    y_val_proba = model.predict_proba(X_val)[:, 1]
    val_report = classification_report(y_val, y_val_pred, output_dict=True)
    val_auc = roc_auc_score(y_val, y_val_proba)

    # Test performance
    y_test_pred = model.predict(X_test)
    y_test_proba = model.predict_proba(X_test)[:, 1]
    test_report = classification_report(y_test, y_test_pred, output_dict=True)
    test_auc = roc_auc_score(y_test, y_test_proba)

    # Log results
    results.append({
        "Model": name,
        "Val_Accuracy": val_report["accuracy"],
        "Val_Precision": val_report["1"]["precision"],
        "Val_Recall": val_report["1"]["recall"],
        "Val_F1": val_report["1"]["f1-score"],
        "Val_ROC-AUC": val_auc,
        "Test_Accuracy": test_report["accuracy"],
        "Test_Precision": test_report["1"]["precision"],
        "Test_Recall": test_report["1"]["recall"],
        "Test_F1": test_report["1"]["f1-score"],
        "Test_ROC-AUC": test_auc
    })

    print(f"\n{name} Validation Results:")
    print(classification_report(y_val, y_val_pred))
    print(f"ROC-AUC (Validation): {val_auc:.4f}")

    print(f"\n{name} Test Results:")
    print(classification_report(y_test, y_test_pred))
    print(f"ROC-AUC (Test): {test_auc:.4f}")
    print("Confusion Matrix (Test):\n", confusion_matrix(y_test, y_test_pred))

# 7. Compare Models
results_df = pd.DataFrame(results).sort_values(by="Val_ROC-AUC", ascending=False)
print("\nModel Comparison (Validation + Test):")
print(results_df)

# 8. Save Comparison & Best Model
os.makedirs("models", exist_ok=True)
results_df.to_csv("models/model_comparison.csv", index=False)
best_model_name = results_df.iloc[0]["Model"]
best_model = models[best_model_name]

model_path = f"models/{best_model_name.replace(' ', '_').lower()}_model.pkl"
joblib.dump(best_model, model_path)

print(f"\nAll models evaluated. Best model: {best_model_name}")
print(f"Saved model to: {model_path}")
print(f"Model comparison saved to: models/model_comparison.csv")